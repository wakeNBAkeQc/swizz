{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakeNBAkeQc/swizz/blob/main/veo2_%2B_tts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/googleapis/python-genai\n",
        "\n",
        "google-genai is an initial Python client library for interacting with Google’s Generative AI APIs.\n",
        "\n",
        "Google Gen AI Python SDK provides an interface for developers to integrate Google’s generative models into their Python applications. It supports the Gemini Developer API and Vertex AI APIs.\n",
        "\n",
        "Installation\n",
        "pip install google-genai\n",
        "Imports\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "Create a client\n",
        "Please run one of the following code blocks to create a client for different services (Gemini Developer API or Vertex AI). Feel free to switch the client and run all the examples to see how it behaves under different APIs.\n",
        "\n",
        "from google import genai\n",
        "\n",
        "# Only run this block for Gemini Developer API\n",
        "client = genai.Client(api_key='GEMINI_API_KEY')\n",
        "from google import genai\n",
        "\n",
        "# Only run this block for Vertex AI API\n",
        "client = genai.Client(\n",
        "    vertexai=True, project='your-project-id', location='us-central1'\n",
        ")\n",
        "(Optional) Using environment variables:\n",
        "\n",
        "You can create a client by configuring the necessary environment variables. Configuration setup instructions depends on whether you’re using the Gemini Developer API or the Gemini API in Vertex AI.\n",
        "\n",
        "Gemini Developer API: Set GOOGLE_API_KEY as shown below:\n",
        "\n",
        "export GOOGLE_API_KEY='your-api-key'\n",
        "Gemini API in Vertex AI: Set GOOGLE_GENAI_USE_VERTEXAI, GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION, as shown below:\n",
        "\n",
        "export GOOGLE_GENAI_USE_VERTEXAI=true\n",
        "export GOOGLE_CLOUD_PROJECT='your-project-id'\n",
        "export GOOGLE_CLOUD_LOCATION='us-central1'\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "API Selection\n",
        "By default, the SDK uses the beta API endpoints provided by Google to support preview features in the APIs. The stable API endpoints can be selected by setting the API version to v1.\n",
        "\n",
        "To set the API version use http_options. For example, to set the API version to v1 for Vertex AI:\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project='your-project-id',\n",
        "    location='us-central1',\n",
        "    http_options=types.HttpOptions(api_version='v1')\n",
        ")\n",
        "To set the API version to v1alpha for the Gemini Developer API:\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Only run this block for Gemini Developer API\n",
        "client = genai.Client(\n",
        "    api_key='GEMINI_API_KEY',\n",
        "    http_options=types.HttpOptions(api_version='v1alpha')\n",
        ")\n",
        "Faster async client option: Aiohttp\n",
        "By default we use httpx for both sync and async client implementations. In order to have faster performance, you may install google-genai[aiohttp]. In Gen AI SDK we configure trust_env=True to match with the default behavior of httpx. Additional args of aiohttp.ClientSession.request() (see _RequestOptions args) can be passed through the following way:\n",
        "\n",
        "http_options = types.HttpOptions(\n",
        "    async_client_args={'cookies': ..., 'ssl': ...},\n",
        ")\n",
        "\n",
        "client=Client(..., http_options=http_options)\n",
        "Proxy\n",
        "Both httpx and aiohttp libraries use urllib.request.getproxies from environment variables. Before client initialization, you may set proxy (and optional SSL_CERT_FILE) by setting the environment variables:\n",
        "\n",
        "export HTTPS_PROXY='http://username:password@proxy_uri:port'\n",
        "export SSL_CERT_FILE='client.pem'\n",
        "If you need socks5 proxy, httpx supports socks5 proxy if you pass it via args to httpx.Client(). You may install httpx[socks] to use it. Then you can pass it through the following way:\n",
        "\n",
        "http_options = types.HttpOptions(\n",
        "    client_args={'proxy': 'socks5://user:pass@host:port'},\n",
        "    async_client_args={'proxy': 'socks5://user:pass@host:port'},\n",
        ")\n",
        "\n",
        "client=Client(..., http_options=http_options)\n",
        "Types\n",
        "Parameter types can be specified as either dictionaries(TypedDict) or Pydantic Models. Pydantic model types are available in the types module.\n",
        "\n",
        "Models\n",
        "The client.models modules exposes model inferencing and model getters. See the ‘Create a client’ section above to initialize a client.\n",
        "\n",
        "Generate Content\n",
        "with text content\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001', contents='Why is the sky blue?'\n",
        ")\n",
        "print(response.text)\n",
        "with uploaded file (Gemini Developer API only)\n",
        "download the file in console.\n",
        "\n",
        "!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "python code.\n",
        "\n",
        "file = client.files.upload(file='a11.txt')\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents=['Could you summarize this file?', file]\n",
        ")\n",
        "print(response.text)\n",
        "How to structure contents argument for generate_content\n",
        "The SDK always converts the inputs to the contents argument into list[types.Content]. The following shows some common ways to provide your inputs.\n",
        "\n",
        "Provide a list[types.Content]\n",
        "This is the canonical way to provide contents, SDK will not do any conversion.\n",
        "\n",
        "Provide a types.Content instance\n",
        "from google.genai import types\n",
        "\n",
        "contents = types.Content(\n",
        "role='user',\n",
        "parts=[types.Part.from_text(text='Why is the sky blue?')]\n",
        ")\n",
        "SDK converts this to\n",
        "\n",
        "[\n",
        "types.Content(\n",
        "    role='user',\n",
        "    parts=[types.Part.from_text(text='Why is the sky blue?')]\n",
        ")\n",
        "]\n",
        "Provide a string\n",
        "contents='Why is the sky blue?'\n",
        "The SDK will assume this is a text part, and it converts this into the following:\n",
        "\n",
        "[\n",
        "types.UserContent(\n",
        "    parts=[\n",
        "    types.Part.from_text(text='Why is the sky blue?')\n",
        "    ]\n",
        ")\n",
        "]\n",
        "Where a types.UserContent is a subclass of types.Content, it sets the role field to be user.\n",
        "\n",
        "Provide a list of string\n",
        "The SDK assumes these are 2 text parts, it converts this into a single content, like the following:\n",
        "\n",
        "[\n",
        "types.UserContent(\n",
        "    parts=[\n",
        "    types.Part.from_text(text='Why is the sky blue?'),\n",
        "    types.Part.from_text(text='Why is the cloud white?'),\n",
        "    ]\n",
        ")\n",
        "]\n",
        "Where a types.UserContent is a subclass of types.Content, the role field in types.UserContent is fixed to be user.\n",
        "\n",
        "Provide a function call part\n",
        "from google.genai import types\n",
        "\n",
        "contents = types.Part.from_function_call(\n",
        "name='get_weather_by_location',\n",
        "args={'location': 'Boston'}\n",
        ")\n",
        "The SDK converts a function call part to a content with a model role:\n",
        "\n",
        "[\n",
        "types.ModelContent(\n",
        "    parts=[\n",
        "    types.Part.from_function_call(\n",
        "        name='get_weather_by_location',\n",
        "        args={'location': 'Boston'}\n",
        "    )\n",
        "    ]\n",
        ")\n",
        "]\n",
        "Where a types.ModelContent is a subclass of types.Content, the role field in types.ModelContent is fixed to be model.\n",
        "\n",
        "Provide a list of function call parts\n",
        "from google.genai import types\n",
        "\n",
        "contents = [\n",
        "types.Part.from_function_call(\n",
        "    name='get_weather_by_location',\n",
        "    args={'location': 'Boston'}\n",
        "),\n",
        "types.Part.from_function_call(\n",
        "    name='get_weather_by_location',\n",
        "    args={'location': 'New York'}\n",
        "),\n",
        "]\n",
        "The SDK converts a list of function call parts to the a content with a model role:\n",
        "\n",
        "[\n",
        "types.ModelContent(\n",
        "    parts=[\n",
        "    types.Part.from_function_call(\n",
        "        name='get_weather_by_location',\n",
        "        args={'location': 'Boston'}\n",
        "    ),\n",
        "    types.Part.from_function_call(\n",
        "        name='get_weather_by_location',\n",
        "        args={'location': 'New York'}\n",
        "    )\n",
        "    ]\n",
        ")\n",
        "]\n",
        "Where a types.ModelContent is a subclass of types.Content, the role field in types.ModelContent is fixed to be model.\n",
        "\n",
        "Provide a non function call part\n",
        "from google.genai import types\n",
        "\n",
        "contents = types.Part.from_uri(\n",
        "file_uri: 'gs://generativeai-downloads/images/scones.jpg',\n",
        "mime_type: 'image/jpeg',\n",
        ")\n",
        "The SDK converts all non function call parts into a content with a user role.\n",
        "\n",
        "[\n",
        "types.UserContent(parts=[\n",
        "    types.Part.from_uri(\n",
        "    file_uri: 'gs://generativeai-downloads/images/scones.jpg',\n",
        "    mime_type: 'image/jpeg',\n",
        "    )\n",
        "])\n",
        "]\n",
        "Provide a list of non function call parts\n",
        "from google.genai import types\n",
        "\n",
        "contents = [\n",
        "types.Part.from_text('What is this image about?'),\n",
        "types.Part.from_uri(\n",
        "    file_uri: 'gs://generativeai-downloads/images/scones.jpg',\n",
        "    mime_type: 'image/jpeg',\n",
        ")\n",
        "]\n",
        "The SDK will convert the list of parts into a content with a user role\n",
        "\n",
        "[\n",
        "types.UserContent(\n",
        "    parts=[\n",
        "    types.Part.from_text('What is this image about?'),\n",
        "    types.Part.from_uri(\n",
        "        file_uri: 'gs://generativeai-downloads/images/scones.jpg',\n",
        "        mime_type: 'image/jpeg',\n",
        "    )\n",
        "    ]\n",
        ")\n",
        "]\n",
        "Mix types in contents\n",
        "You can also provide a list of types.ContentUnion. The SDK leaves items of types.Content as is, it groups consecutive non function call parts into a single types.UserContent, and it groups consecutive function call parts into a single types.ModelContent.\n",
        "\n",
        "If you put a list within a list, the inner list can only contain types.PartUnion items. The SDK will convert the inner list into a single types.UserContent.\n",
        "\n",
        "System Instructions and Other Configs\n",
        "The output of the model can be influenced by several optional settings available in generate_content’s config parameter. For example, increasing max_output_tokens is essential for longer model responses. To make a model more deterministic, lowering the temperature parameter reduces randomness, with values near 0 minimizing variability. Capabilities and parameter defaults for each model is shown in the Vertex AI docs and Gemini API docs respectively.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='high',\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction='I say high, you say low',\n",
        "        max_output_tokens=3,\n",
        "        temperature=0.3,\n",
        "    ),\n",
        ")\n",
        "print(response.text)\n",
        "Typed Config\n",
        "All API methods support Pydantic types for parameters as well as dictionaries. You can get the type from google.genai.types.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents=types.Part.from_text(text='Why is the sky blue?'),\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,\n",
        "        stop_sequences=['STOP!'],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "List Base Models\n",
        "To retrieve tuned models, see: List Tuned Models\n",
        "\n",
        "for model in client.models.list():\n",
        "    print(model)\n",
        "pager = client.models.list(config={'page_size': 10})\n",
        "print(pager.page_size)\n",
        "print(pager[0])\n",
        "pager.next_page()\n",
        "print(pager[0])\n",
        "List Base Models (Asynchronous)\n",
        "async for job in await client.aio.models.list():\n",
        "    print(job)\n",
        "async_pager = await client.aio.models.list(config={'page_size': 10})\n",
        "print(async_pager.page_size)\n",
        "print(async_pager[0])\n",
        "await async_pager.next_page()\n",
        "print(async_pager[0])\n",
        "Safety Settings\n",
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='Say something bad.',\n",
        "    config=types.GenerateContentConfig(\n",
        "        safety_settings=[\n",
        "            types.SafetySetting(\n",
        "                category='HARM_CATEGORY_HATE_SPEECH',\n",
        "                threshold='BLOCK_ONLY_HIGH',\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "print(response.text)\n",
        "Function Calling\n",
        "Automatic Python function Support:\n",
        "\n",
        "You can pass a Python function directly and it will be automatically called and responded.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "      location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    return 'sunny'\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='What is the weather like in Boston?',\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Disabling automatic function calling\n",
        "If you pass in a python function as a tool directly, and do not want automatic function calling, you can disable automatic function calling as follows:\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='What is the weather like in Boston?',\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(\n",
        "            disable=True\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "With automatic function calling disabled, you will get a list of function call parts in the response:\n",
        "\n",
        "Manually declare and invoke a function for function calling\n",
        "If you don’t want to use the automatic function support, you can manually declare the function and invoke it.\n",
        "\n",
        "The following example shows how to declare a function and pass it as a tool. Then you will receive a function call part in the response.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "function = types.FunctionDeclaration(\n",
        "    name='get_current_weather',\n",
        "    description='Get the current weather in a given location',\n",
        "    parameters=types.Schema(\n",
        "        type='OBJECT',\n",
        "        properties={\n",
        "            'location': types.Schema(\n",
        "                type='STRING',\n",
        "                description='The city and state, e.g. San Francisco, CA',\n",
        "            ),\n",
        "        },\n",
        "        required=['location'],\n",
        "    ),\n",
        ")\n",
        "\n",
        "tool = types.Tool(function_declarations=[function])\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='What is the weather like in Boston?',\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[tool],\n",
        "    ),\n",
        ")\n",
        "print(response.function_calls[0])\n",
        "After you receive the function call part from the model, you can invoke the function and get the function response. And then you can pass the function response to the model. The following example shows how to do it for a simple function invocation.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "user_prompt_content = types.Content(\n",
        "    role='user',\n",
        "    parts=[types.Part.from_text(text='What is the weather like in Boston?')],\n",
        ")\n",
        "function_call_part = response.function_calls[0]\n",
        "function_call_content = response.candidates[0].content\n",
        "\n",
        "\n",
        "try:\n",
        "    function_result = get_current_weather(\n",
        "        **function_call_part.function_call.args\n",
        "    )\n",
        "    function_response = {'result': function_result}\n",
        "except (\n",
        "    Exception\n",
        ") as e:  # instead of raising the exception, you can let the model handle it\n",
        "    function_response = {'error': str(e)}\n",
        "\n",
        "\n",
        "function_response_part = types.Part.from_function_response(\n",
        "    name=function_call_part.name,\n",
        "    response=function_response,\n",
        ")\n",
        "function_response_content = types.Content(\n",
        "    role='tool', parts=[function_response_part]\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents=[\n",
        "        user_prompt_content,\n",
        "        function_call_content,\n",
        "        function_response_content,\n",
        "    ],\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[tool],\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Function calling with ANY tools config mode\n",
        "If you configure function calling mode to be ANY, then the model will always return function call parts. If you also pass a python function as a tool, by default the SDK will perform automatic function calling until the remote calls exceed the maximum remote call for automatic function calling (default to 10 times).\n",
        "\n",
        "If you’d like to disable automatic function calling in ANY mode:\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    return \"sunny\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    contents=\"What is the weather like in Boston?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(\n",
        "            disable=True\n",
        "        ),\n",
        "        tool_config=types.ToolConfig(\n",
        "            function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "If you’d like to set x number of automatic function call turns, you can configure the maximum remote calls to be x + 1. Assuming you prefer 1 turn for automatic function calling:\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    return \"sunny\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    contents=\"What is the weather like in Boston?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        automatic_function_calling=types.AutomaticFunctionCallingConfig(\n",
        "            maximum_remote_calls=2\n",
        "        ),\n",
        "        tool_config=types.ToolConfig(\n",
        "            function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "JSON Response Schema\n",
        "Pydantic Model Schema support\n",
        "Schemas can be provided as Pydantic Models.\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "class CountryInfo(BaseModel):\n",
        "    name: str\n",
        "    population: int\n",
        "    capital: str\n",
        "    continent: str\n",
        "    gdp: int\n",
        "    official_language: str\n",
        "    total_area_sq_mi: int\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='Give me information for the United States.',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type='application/json',\n",
        "        response_schema=CountryInfo,\n",
        "    ),\n",
        ")\n",
        "print(response.text)\n",
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='Give me information for the United States.',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type='application/json',\n",
        "        response_schema={\n",
        "            'required': [\n",
        "                'name',\n",
        "                'population',\n",
        "                'capital',\n",
        "                'continent',\n",
        "                'gdp',\n",
        "                'official_language',\n",
        "                'total_area_sq_mi',\n",
        "            ],\n",
        "            'properties': {\n",
        "                'name': {'type': 'STRING'},\n",
        "                'population': {'type': 'INTEGER'},\n",
        "                'capital': {'type': 'STRING'},\n",
        "                'continent': {'type': 'STRING'},\n",
        "                'gdp': {'type': 'INTEGER'},\n",
        "                'official_language': {'type': 'STRING'},\n",
        "                'total_area_sq_mi': {'type': 'INTEGER'},\n",
        "            },\n",
        "            'type': 'OBJECT',\n",
        "        },\n",
        "    ),\n",
        ")\n",
        "print(response.text)\n",
        "Enum Response Schema\n",
        "Text Response\n",
        "You can set response_mime_type to ‘text/x.enum’ to return one of those enum values as the response.\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "class InstrumentEnum(Enum):\n",
        "    PERCUSSION = 'Percussion'\n",
        "    STRING = 'String'\n",
        "    WOODWIND = 'Woodwind'\n",
        "    BRASS = 'Brass'\n",
        "    KEYBOARD = 'Keyboard'\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='What instrument plays multiple notes at once?',\n",
        "    config={\n",
        "        'response_mime_type': 'text/x.enum',\n",
        "        'response_schema': InstrumentEnum,\n",
        "    },\n",
        ")\n",
        "print(response.text)\n",
        "JSON Response\n",
        "You can also set response_mime_type to ‘application/json’, the response will be identical but in quotes.\n",
        "\n",
        "class InstrumentEnum(Enum):\n",
        "    PERCUSSION = 'Percussion'\n",
        "    STRING = 'String'\n",
        "    WOODWIND = 'Woodwind'\n",
        "    BRASS = 'Brass'\n",
        "    KEYBOARD = 'Keyboard'\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='What instrument plays multiple notes at once?',\n",
        "    config={\n",
        "        'response_mime_type': 'application/json',\n",
        "        'response_schema': InstrumentEnum,\n",
        "    },\n",
        ")\n",
        "print(response.text)\n",
        "Generate Content (Synchronous Streaming)\n",
        "Generate content in a streaming format so that the model outputs streams back to you, rather than being returned as one chunk.\n",
        "\n",
        "Streaming for text content\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-001', contents='Tell me a story in 300 words.'\n",
        "):\n",
        "    print(chunk.text, end='')\n",
        "Streaming for image content\n",
        "If your image is stored in Google Cloud Storage, you can use the from_uri class method to create a Part object.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents=[\n",
        "        'What is this image about?',\n",
        "        types.Part.from_uri(\n",
        "            file_uri='gs://generativeai-downloads/images/scones.jpg',\n",
        "            mime_type='image/jpeg',\n",
        "        ),\n",
        "    ],\n",
        "):\n",
        "    print(chunk.text, end='')\n",
        "If your image is stored in your local file system, you can read it in as bytes data and use the from_bytes class method to create a Part object.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "YOUR_IMAGE_PATH = 'your_image_path'\n",
        "YOUR_IMAGE_MIME_TYPE = 'your_image_mime_type'\n",
        "with open(YOUR_IMAGE_PATH, 'rb') as f:\n",
        "    image_bytes = f.read()\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents=[\n",
        "        'What is this image about?',\n",
        "        types.Part.from_bytes(data=image_bytes, mime_type=YOUR_IMAGE_MIME_TYPE),\n",
        "    ],\n",
        "):\n",
        "    print(chunk.text, end='')\n",
        "Generate Content (Asynchronous Non-Streaming)\n",
        "client.aio exposes all the analogous async methods that are available on client. Note that it applies to all the modules.\n",
        "\n",
        "For example, client.aio.models.generate_content is the async version of client.models.generate_content\n",
        "\n",
        "response = await client.aio.models.generate_content(\n",
        "    model='gemini-2.0-flash-001', contents='Tell me a story in 300 words.'\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Generate Content (Asynchronous Streaming)\n",
        "async for chunk in await client.aio.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-001', contents='Tell me a story in 300 words.'\n",
        "):\n",
        "    print(chunk.text, end='')\n",
        "Count Tokens and Compute Tokens\n",
        "response = client.models.count_tokens(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='why is the sky blue?',\n",
        ")\n",
        "print(response)\n",
        "Compute Tokens\n",
        "Compute tokens is only supported in Vertex AI.\n",
        "\n",
        "response = client.models.compute_tokens(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='why is the sky blue?',\n",
        ")\n",
        "print(response)\n",
        "Count Tokens (Asynchronous)\n",
        "response = await client.aio.models.count_tokens(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='why is the sky blue?',\n",
        ")\n",
        "print(response)\n",
        "Embed Content\n",
        "response = client.models.embed_content(\n",
        "    model='text-embedding-004',\n",
        "    contents='why is the sky blue?',\n",
        ")\n",
        "print(response)\n",
        "from google.genai import types\n",
        "\n",
        "# multiple contents with config\n",
        "response = client.models.embed_content(\n",
        "    model='text-embedding-004',\n",
        "    contents=['why is the sky blue?', 'What is your age?'],\n",
        "    config=types.EmbedContentConfig(output_dimensionality=10),\n",
        ")\n",
        "\n",
        "print(response)\n",
        "Imagen\n",
        "Generate Image\n",
        "Support for generate image in Gemini Developer API is behind an allowlist\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "# Generate Image\n",
        "response1 = client.models.generate_images(\n",
        "    model='imagen-3.0-generate-002',\n",
        "    prompt='An umbrella in the foreground, and a rainy night sky in the background',\n",
        "    config=types.GenerateImagesConfig(\n",
        "        number_of_images=1,\n",
        "        include_rai_reason=True,\n",
        "        output_mime_type='image/jpeg',\n",
        "    ),\n",
        ")\n",
        "response1.generated_images[0].image.show()\n",
        "Upscale image is only supported in Vertex AI.\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "# Upscale the generated image from above\n",
        "response2 = client.models.upscale_image(\n",
        "    model='imagen-3.0-generate-002',\n",
        "    image=response1.generated_images[0].image,\n",
        "    upscale_factor='x2',\n",
        "    config=types.UpscaleImageConfig(\n",
        "        include_rai_reason=True,\n",
        "        output_mime_type='image/jpeg',\n",
        "    ),\n",
        ")\n",
        "response2.generated_images[0].image.show()\n",
        "Edit Image\n",
        "Edit image uses a separate model from generate and upscale.\n",
        "\n",
        "Edit image is only supported in Vertex AI.\n",
        "\n",
        "# Edit the generated image from above\n",
        "from google.genai import types\n",
        "from google.genai.types import RawReferenceImage, MaskReferenceImage\n",
        "\n",
        "raw_ref_image = RawReferenceImage(\n",
        "    reference_id=1,\n",
        "    reference_image=response1.generated_images[0].image,\n",
        ")\n",
        "\n",
        "# Model computes a mask of the background\n",
        "mask_ref_image = MaskReferenceImage(\n",
        "    reference_id=2,\n",
        "    config=types.MaskReferenceConfig(\n",
        "        mask_mode='MASK_MODE_BACKGROUND',\n",
        "        mask_dilation=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response3 = client.models.edit_image(\n",
        "    model='imagen-3.0-capability-001',\n",
        "    prompt='Sunlight and clear sky',\n",
        "    reference_images=[raw_ref_image, mask_ref_image],\n",
        "    config=types.EditImageConfig(\n",
        "        edit_mode='EDIT_MODE_INPAINT_INSERTION',\n",
        "        number_of_images=1,\n",
        "        include_rai_reason=True,\n",
        "        output_mime_type='image/jpeg',\n",
        "    ),\n",
        ")\n",
        "response3.generated_images[0].image.show()\n",
        "Veo\n",
        "Generate Videos\n",
        "Support for generate videos in Vertex and Gemini Developer API is behind an allowlist\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "# Create operation\n",
        "operation = client.models.generate_videos(\n",
        "    model='veo-2.0-generate-001',\n",
        "    prompt='A neon hologram of a cat driving at top speed',\n",
        "    config=types.GenerateVideosConfig(\n",
        "        number_of_videos=1,\n",
        "        fps=24,\n",
        "        duration_seconds=5,\n",
        "        enhance_prompt=True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Poll operation\n",
        "while not operation.done:\n",
        "    time.sleep(20)\n",
        "    operation = client.operations.get(operation)\n",
        "\n",
        "video = operation.result.generated_videos[0].video\n",
        "video.show()\n",
        "Chats\n",
        "Create a chat session to start a multi-turn conversations with the model. Then, use chat.send_message function multiple times within the same chat session so that it can reflect on its previous responses (i.e., engage in an ongoing conversation). See the ‘Create a client’ section above to initialize a client.\n",
        "\n",
        "Send Message (Synchronous Non-Streaming)\n",
        "chat = client.chats.create(model='gemini-2.0-flash-001')\n",
        "response = chat.send_message('tell me a story')\n",
        "print(response.text)\n",
        "response = chat.send_message('summarize the story you told me in 1 sentence')\n",
        "print(response.text)\n",
        "Send Message (Synchronous Streaming)\n",
        "chat = client.chats.create(model='gemini-2.0-flash-001')\n",
        "for chunk in chat.send_message_stream('tell me a story'):\n",
        "    print(chunk.text, end='')  # end='' is optional, for demo purposes.\n",
        "Send Message (Asynchronous Non-Streaming)\n",
        "chat = client.aio.chats.create(model='gemini-2.0-flash-001')\n",
        "response = await chat.send_message('tell me a story')\n",
        "print(response.text)\n",
        "Send Message (Asynchronous Streaming)\n",
        "chat = client.aio.chats.create(model='gemini-2.0-flash-001')\n",
        "async for chunk in await chat.send_message_stream('tell me a story'):\n",
        "    print(chunk.text, end='') # end='' is optional, for demo purposes.\n",
        "Files\n",
        "Files are only supported in Gemini Developer API. See the ‘Create a client’ section above to initialize a client.\n",
        "\n",
        "gsutil cp gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf .\n",
        "gsutil cp gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf .\n",
        "Upload\n",
        "file1 = client.files.upload(file='2312.11805v3.pdf')\n",
        "file2 = client.files.upload(file='2403.05530.pdf')\n",
        "\n",
        "print(file1)\n",
        "print(file2)\n",
        "Get\n",
        "file1 = client.files.upload(file='2312.11805v3.pdf')\n",
        "file_info = client.files.get(name=file1.name)\n",
        "Delete\n",
        "file3 = client.files.upload(file='2312.11805v3.pdf')\n",
        "\n",
        "client.files.delete(name=file3.name)\n",
        "Caches\n",
        "client.caches contains the control plane APIs for cached content.\n",
        "See the ‘Create a client’ section above to initialize a client.\n",
        "\n",
        "Create\n",
        "from google.genai import types\n",
        "\n",
        "if client.vertexai:\n",
        "    file_uris = [\n",
        "        'gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf',\n",
        "        'gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf',\n",
        "    ]\n",
        "else:\n",
        "    file_uris = [file1.uri, file2.uri]\n",
        "\n",
        "cached_content = client.caches.create(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    config=types.CreateCachedContentConfig(\n",
        "        contents=[\n",
        "            types.Content(\n",
        "                role='user',\n",
        "                parts=[\n",
        "                    types.Part.from_uri(\n",
        "                        file_uri=file_uris[0], mime_type='application/pdf'\n",
        "                    ),\n",
        "                    types.Part.from_uri(\n",
        "                        file_uri=file_uris[1],\n",
        "                        mime_type='application/pdf',\n",
        "                    ),\n",
        "                ],\n",
        "            )\n",
        "        ],\n",
        "        system_instruction='What is the sum of the two pdfs?',\n",
        "        display_name='test cache',\n",
        "        ttl='3600s',\n",
        "    ),\n",
        ")\n",
        "Get\n",
        "cached_content = client.caches.get(name=cached_content.name)\n",
        "Generate Content with Caches\n",
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    contents='Summarize the pdfs',\n",
        "    config=types.GenerateContentConfig(\n",
        "        cached_content=cached_content.name,\n",
        "    ),\n",
        ")\n",
        "print(response.text)\n",
        "Tunings\n",
        "client.tunings contains tuning job APIs and supports supervised fine tuning through tune. Only supported in Vertex AI. See the ‘Create a client’ section above to initialize a client.\n",
        "\n",
        "Tune\n",
        "Vertex AI supports tuning from GCS source\n",
        "\n",
        "from google.genai import types\n",
        "\n",
        "model = 'gemini-2.0-flash-001'\n",
        "training_dataset = types.TuningDataset(\n",
        "    gcs_uri='gs://cloud-samples-data/ai-platform/generative_ai/gemini-1_5/text/sft_train_data.jsonl',\n",
        ")\n",
        "from google.genai import types\n",
        "\n",
        "tuning_job = client.tunings.tune(\n",
        "    base_model=model,\n",
        "    training_dataset=training_dataset,\n",
        "    config=types.CreateTuningJobConfig(\n",
        "        epoch_count=1, tuned_model_display_name='test_dataset_examples model'\n",
        "    ),\n",
        ")\n",
        "print(tuning_job)\n",
        "Get Tuning Job\n",
        "tuning_job = client.tunings.get(name=tuning_job.name)\n",
        "print(tuning_job)\n",
        "import time\n",
        "\n",
        "completed_states = set(\n",
        "    [\n",
        "        'JOB_STATE_SUCCEEDED',\n",
        "        'JOB_STATE_FAILED',\n",
        "        'JOB_STATE_CANCELLED',\n",
        "    ]\n",
        ")\n",
        "\n",
        "while tuning_job.state not in completed_states:\n",
        "    print(tuning_job.state)\n",
        "    tuning_job = client.tunings.get(name=tuning_job.name)\n",
        "    time.sleep(10)\n",
        "Use Tuned Model\n",
        "response = client.models.generate_content(\n",
        "    model=tuning_job.tuned_model.endpoint,\n",
        "    contents='why is the sky blue?',\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "Get Tuned Model\n",
        "tuned_model = client.models.get(model=tuning_job.tuned_model.model)\n",
        "print(tuned_model)\n",
        "Update Tuned Model\n",
        "from google.genai import types\n",
        "\n",
        "tuned_model = client.models.update(\n",
        "    model=tuning_job.tuned_model.model,\n",
        "    config=types.UpdateModelConfig(\n",
        "        display_name='my tuned model', description='my tuned model description'\n",
        "    ),\n",
        ")\n",
        "print(tuned_model)\n",
        "List Tuned Models\n",
        "To retrieve base models, see: List Base Models\n",
        "\n",
        "for model in client.models.list(config={'page_size': 10, 'query_base': False}}):\n",
        "    print(model)\n",
        "pager = client.models.list(config={'page_size': 10, 'query_base': False}})\n",
        "print(pager.page_size)\n",
        "print(pager[0])\n",
        "pager.next_page()\n",
        "print(pager[0])\n",
        "List Tuned Models (Asynchronous)\n",
        "async for job in await client.aio.models.list(config={'page_size': 10, 'query_base': False}}):\n",
        "    print(job)\n",
        "async_pager = await client.aio.models.list(config={'page_size': 10, 'query_base': False}})\n",
        "print(async_pager.page_size)\n",
        "print(async_pager[0])\n",
        "await async_pager.next_page()\n",
        "print(async_pager[0])\n",
        "Update Tuned Model\n",
        "model = pager[0]\n",
        "\n",
        "model = client.models.update(\n",
        "    model=model.name,\n",
        "    config=types.UpdateModelConfig(\n",
        "        display_name='my tuned model', description='my tuned model description'\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(model)\n",
        "List Tuning Jobs\n",
        "for job in client.tunings.list(config={'page_size': 10}):\n",
        "    print(job)\n",
        "pager = client.tunings.list(config={'page_size': 10})\n",
        "print(pager.page_size)\n",
        "print(pager[0])\n",
        "pager.next_page()\n",
        "print(pager[0])\n",
        "List Tuning Jobs (Asynchronous):\n",
        "\n",
        "async for job in await client.aio.tunings.list(config={'page_size': 10}):\n",
        "    print(job)\n",
        "async_pager = await client.aio.tunings.list(config={'page_size': 10})\n",
        "print(async_pager.page_size)\n",
        "print(async_pager[0])\n",
        "await async_pager.next_page()\n",
        "print(async_pager[0])\n",
        "Batch Prediction\n",
        "Create a batch job. See the ‘Create a client’ section above to initialize a client.\n",
        "\n",
        "Create\n",
        "Vertex AI client support using a BigQuery table or a GCS file as the source.\n",
        "\n",
        "# Specify model and source file only, destination and job display name will be auto-populated\n",
        "job = client.batches.create(\n",
        "    model='gemini-2.0-flash-001',\n",
        "    src='bq://my-project.my-dataset.my-table',  # or gcs://my-bucket/my-file.jsonl\n",
        ")\n",
        "Gemini Developer API client:\n",
        "\n",
        "# Create a batch job with inlined requests\n",
        "batch_job = client.batches.create(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    src=[{\n",
        "      \"contents\": [{\n",
        "        \"parts\": [{\n",
        "          \"text\": \"Hello!\",\n",
        "        }],\n",
        "       \"role\": \"user\",\n",
        "     }],\n",
        "     \"config:\": {\"response_modalities\": [\"text\"]},\n",
        "    }],\n",
        ")\n",
        "In order to create a batch job with a file. Need to upload a jsonl file. For example myrequests.json:\n",
        "\n",
        "# Get a job by name\n",
        "job = client.batches.get(name=job.name)\n",
        "\n",
        "job.state\n",
        "completed_states = set(\n",
        "    [\n",
        "        'JOB_STATE_SUCCEEDED',\n",
        "        'JOB_STATE_FAILED',\n",
        "        'JOB_STATE_CANCELLED',\n",
        "        'JOB_STATE_PAUSED',\n",
        "    ]\n",
        ")\n",
        "\n",
        "while job.state not in completed_states:\n",
        "    print(job.state)\n",
        "    job = client.batches.get(name=job.name)\n",
        "    time.sleep(30)\n",
        "\n",
        "job\n",
        "List\n",
        "from google.genai import types\n",
        "\n",
        "for job in client.batches.list(config=types.ListBatchJobsConfig(page_size=10)):\n",
        "    print(job)\n",
        "List Batch Jobs with Pager\n",
        "from google.genai import types\n",
        "\n",
        "pager = client.batches.list(config=types.ListBatchJobsConfig(page_size=10))\n",
        "print(pager.page_size)\n",
        "print(pager[0])\n",
        "pager.next_page()\n",
        "print(pager[0])\n",
        "List Batch Jobs (Asynchronous)\n",
        "from google.genai import types\n",
        "\n",
        "async for job in await client.aio.batches.list(\n",
        "    config=types.ListBatchJobsConfig(page_size=10)\n",
        "):\n",
        "    print(job)"
      ],
      "metadata": {
        "id": "O07C84j19BnH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5CerfP18A77"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Please ensure you have imported a Gemini API key from AI Studio.\n",
        "You can do this directly in the Secrets tab on the left.\n",
        "\n",
        "After doing so, please run the setup cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elJreHXW8A7-",
        "outputId": "febfd68b-cb93-4bea-cc69-29b9e8bfd5a4"
      },
      "source": [
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"veo2\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIzDRMSE8A8A"
      },
      "source": [
        "# Generated Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buq9aqpI8A8C"
      },
      "source": [
        "\"\"\"\n",
        "To run this code you need to install the following dependencies:\n",
        "pip install google-genai pillow\n",
        "\"\"\"\n",
        "import time\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "MODEL = \"veo-2.0-generate-001\"\n",
        "\n",
        "client = genai.Client(\n",
        "    http_options={\"api_version\": \"v1beta\"},\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "\n",
        "video_config = types.GenerateVideosConfig(\n",
        "    aspect_ratio=\"16:9\", # supported values: \"16:9\" or \"16:10\"\n",
        "    number_of_videos=1, # supported values: 1 - 4\n",
        "    duration_seconds=8, # supported values: 5 - 8\n",
        "    person_generation=\"ALLOW_ALL\",\n",
        ")\n",
        "\n",
        "def generate():\n",
        "    operation = client.models.generate_videos(\n",
        "        model=MODEL,\n",
        "        prompt=\"\"\"INSERT_INPUT_HERE\"\"\",\n",
        "        config=video_config,\n",
        "    )\n",
        "\n",
        "    # Waiting for the video(s) to be generated\n",
        "    while not operation.done:\n",
        "        print(\"Video has not been generated yet. Check again in 10 seconds...\")\n",
        "        time.sleep(10)\n",
        "        operation = client.operations.get(operation)\n",
        "        print(operation)\n",
        "\n",
        "    result = operation.result\n",
        "    if not result:\n",
        "        print(\"Error occurred while generating video.\")\n",
        "        return\n",
        "\n",
        "    generated_videos = result.generated_videos\n",
        "    print(generated_videos)\n",
        "    if not generated_videos:\n",
        "        print(\"No videos were generated.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Generated {len(generated_videos)} video(s).\")\n",
        "    for n, generated_video in enumerate(generated_videos):\n",
        "        print(f\"Video has been generated: {generated_video.video.uri}\")\n",
        "        client.files.download(file=generated_video.video)\n",
        "        generated_video.video.save(f\"video_{n}.mp4\") # Saves the video(s)\n",
        "        print(f\"Video {generated_video.video.uri} has been downloaded to video_{n}.mp4.\")\n",
        "        display(generated_video.video.show())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "607ce175"
      },
      "source": [
        "# Task\n",
        "Create a Gradio interface that uses the existing `generate` function to create a video from a text prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4289bb"
      },
      "source": [
        "## Install gradio\n",
        "\n",
        "### Subtask:\n",
        "Install the Gradio library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf2356a"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the Gradio library. I will use pip to install it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9623065f",
        "outputId": "a23e979f-bc01-4795-d111-a3e1829ac45c"
      },
      "source": [
        "!pip install gradio"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.43.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.10)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.16.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78a66e3f"
      },
      "source": [
        "## Import necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Import `gradio` and the existing `generate` function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5bf837f"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the gradio library and ensure the generate function is accessible for the Gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "734b4667"
      },
      "source": [
        "import gradio as gr\n",
        "# The generate function is defined in a previous cell and is available in the global scope."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4306ab0d"
      },
      "source": [
        "## Define the gradio interface\n",
        "\n",
        "### Subtask:\n",
        "Create a Gradio interface that takes a text prompt as input and outputs the generated video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09ce3dd"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Gradio interface using the existing `generate` function, a text input, and a video output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "18f553ce",
        "outputId": "e922639b-2dcc-4eaf-e493-499dc63a8f1f"
      },
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=gr.Textbox(label=\"Enter a text prompt for video generation:\"),\n",
        "    outputs=gr.Video(label=\"Generated Video\"),\n",
        "    title=\"Veo 2.0 Video Generator\",\n",
        "    description=\"Generate videos from text prompts using the Veo 2.0 model.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-885694820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m iface = gr.Interface(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Enter a text prompt for video generation:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Generated Video\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Veo 2.0 Video Generator\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31c4efda"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the `generate` function is not defined in the current scope. The `generate` function was defined in a previous cell but was not executed. I need to re-execute the cell that defines the `generate` function before creating the Gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKnqT-4m_bBj"
      },
      "source": [
        "\"\"\"\n",
        "To run this code you need to install the following dependencies:\n",
        "pip install google-genai pillow\n",
        "\"\"\"\n",
        "import time\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "MODEL = \"veo-2.0-generate-001\"\n",
        "\n",
        "client = genai.Client(\n",
        "    http_options={\"api_version\": \"v1beta\"},\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "\n",
        "video_config = types.GenerateVideosConfig(\n",
        "    aspect_ratio=\"16:9\", # supported values: \"16:9\" or \"16:10\"\n",
        "    number_of_videos=1, # supported values: 1 - 4\n",
        "    duration_seconds=8, # supported values: 5 - 8\n",
        "    person_generation=\"ALLOW_ALL\",\n",
        ")\n",
        "\n",
        "def generate(prompt):\n",
        "    operation = client.models.generate_videos(\n",
        "        model=MODEL,\n",
        "        prompt=prompt,\n",
        "        config=video_config,\n",
        "    )\n",
        "\n",
        "    # Waiting for the video(s) to be generated\n",
        "    while not operation.done:\n",
        "        print(\"Video has not been generated yet. Check again in 10 seconds...\")\n",
        "        time.sleep(10)\n",
        "        operation = client.operations.get(operation)\n",
        "        print(operation)\n",
        "\n",
        "    result = operation.result\n",
        "    if not result:\n",
        "        print(\"Error occurred while generating video.\")\n",
        "        return\n",
        "\n",
        "    generated_videos = result.generated_videos\n",
        "    print(generated_videos)\n",
        "    if not generated_videos:\n",
        "        print(\"No videos were generated.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Generated {len(generated_videos)} video(s).\")\n",
        "    for n, generated_video in enumerate(generated_videos):\n",
        "        print(f\"Video has been generated: {generated_video.video.uri}\")\n",
        "        client.files.download(file=generated_video.video)\n",
        "        generated_video.video.save(f\"video_{n}.mp4\") # Saves the video(s)\n",
        "        print(f\"Video {generated_video.video.uri} has been downloaded to video_{n}.mp4.\")\n",
        "        return f\"video_{n}.mp4\"\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53973fd3"
      },
      "source": [
        "**Reasoning**:\n",
        "The `generate` function is now defined. I can proceed with creating and launching the Gradio interface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "43d2b4a0",
        "outputId": "f57be31a-e0d3-4ce8-8c6b-f952e0125560"
      },
      "source": [
        "iface = gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=gr.Textbox(label=\"Enter a text prompt for video generation:\"),\n",
        "    outputs=gr.Video(label=\"Generated Video\"),\n",
        "    title=\"Veo 2.0 Video Generator\",\n",
        "    description=\"Generate videos from text prompts using the Veo 2.0 model.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c884d934daa3702e5b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c884d934daa3702e5b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb3c3945"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Gradio library was successfully installed and confirmed to be available in the environment.\n",
        "*   The necessary libraries, `gradio` and the `generate` function (defined in a prior step), were successfully imported.\n",
        "*   A Gradio interface was successfully created and launched, configured to take a text prompt as input and display the generated video as output.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The created Gradio interface provides a user-friendly way to interact with the video generation model.\n",
        "*   Consider adding more options to the Gradio interface, such as selecting video duration or aspect ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import mimetypes\n",
        "import os\n",
        "import struct\n",
        "import gradio as gr\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration de l'API et du Drive ---\n",
        "# Assurez-vous que 'veo2' est le nom de votre secret dans Colab\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"veo2\")\n",
        "try:\n",
        "    drive.mount(\"/content/drive\")\n",
        "    os.chdir(\"/content/drive/MyDrive/Google AI Studio\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du montage de Drive : {e}\")\n",
        "\n",
        "# Clients pour les deux API\n",
        "client_genai = genai.Client(\n",
        "    http_options={\"api_version\": \"v1beta\"},\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "client_tts = genai.Client(\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "\n",
        "# --- Fonction de génération de vidéo ---\n",
        "MODEL_VEO = \"veo-2.0-generate-001\"\n",
        "video_config = types.GenerateVideosConfig(\n",
        "    aspect_ratio=\"16:9\",\n",
        "    number_of_videos=1,\n",
        "    duration_seconds=8,\n",
        "    person_generation=\"ALLOW_ALL\",\n",
        ")\n",
        "\n",
        "def generate_video(prompt):\n",
        "    operation = client_genai.models.generate_videos(\n",
        "        model=MODEL_VEO,\n",
        "        prompt=prompt,\n",
        "        config=video_config,\n",
        "    )\n",
        "    while not operation.done:\n",
        "        print(\"Vidéo en cours de génération...\")\n",
        "        time.sleep(10)\n",
        "        operation = client_genai.operations.get(operation)\n",
        "\n",
        "    result = operation.result\n",
        "    if not result or not result.generated_videos:\n",
        "        return None\n",
        "\n",
        "    generated_video = result.generated_videos[0]\n",
        "    file_path = \"generated_video.mp4\"\n",
        "    client_genai.files.download(file=generated_video.video)\n",
        "    generated_video.video.save(file_path)\n",
        "    return file_path\n",
        "\n",
        "# --- Fonctions pour le TTS (tirées de votre script) ---\n",
        "def save_binary_file(file_name, data):\n",
        "    f = open(file_name, \"wb\")\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "    return file_name\n",
        "\n",
        "def convert_to_wav(audio_data: bytes, mime_type: str) -> bytes:\n",
        "    parameters = parse_audio_mime_type(mime_type)\n",
        "    bits_per_sample = parameters[\"bits_per_sample\"]\n",
        "    sample_rate = parameters[\"rate\"]\n",
        "    num_channels = 1\n",
        "    data_size = len(audio_data)\n",
        "    bytes_per_sample = bits_per_sample // 8\n",
        "    block_align = num_channels * bytes_per_sample\n",
        "    byte_rate = sample_rate * block_align\n",
        "    chunk_size = 36 + data_size\n",
        "    header = struct.pack(\n",
        "        \"<4sI4s4sIHHIIHH4sI\",\n",
        "        b\"RIFF\",\n",
        "        chunk_size,\n",
        "        b\"WAVE\",\n",
        "        b\"fmt \",\n",
        "        16,\n",
        "        1,\n",
        "        num_channels,\n",
        "        sample_rate,\n",
        "        byte_rate,\n",
        "        block_align,\n",
        "        bits_per_sample,\n",
        "        b\"data\",\n",
        "        data_size\n",
        "    )\n",
        "    return header + audio_data\n",
        "\n",
        "def parse_audio_mime_type(mime_type: str) -> dict[str, int | None]:\n",
        "    bits_per_sample = 16\n",
        "    rate = 24000\n",
        "    parts = mime_type.split(\";\")\n",
        "    for param in parts:\n",
        "        param = param.strip()\n",
        "        if param.lower().startswith(\"rate=\"):\n",
        "            try:\n",
        "                rate_str = param.split(\"=\", 1)[1]\n",
        "                rate = int(rate_str)\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "        elif param.startswith(\"audio/L\"):\n",
        "            try:\n",
        "                bits_per_sample = int(param.split(\"L\", 1)[1])\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "    return {\"bits_per_sample\": bits_per_sample, \"rate\": rate}\n",
        "\n",
        "def generate_tts_dialog(prompt_text, speaker1_name, speaker2_name, speaker1_voice, speaker2_voice):\n",
        "    \"\"\"Génère un dialogue TTS en utilisant l'API Gemini et le stocke.\"\"\"\n",
        "    model = \"gemini-2.5-flash-preview-tts\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=f\"\"\"Read aloud in a neutral, calm voice\n",
        "{speaker1_name}: {prompt_text}\n",
        "{speaker2_name}: (Réponse de l'interlocuteur 2)\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        response_modalities=[\"audio\"],\n",
        "        speech_config=types.SpeechConfig(\n",
        "            multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(\n",
        "                speaker_voice_configs=[\n",
        "                    types.SpeakerVoiceConfig(\n",
        "                        speaker=speaker1_name,\n",
        "                        voice_config=types.VoiceConfig(\n",
        "                            prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                voice_name=speaker1_voice\n",
        "                            )\n",
        "                        ),\n",
        "                    ),\n",
        "                    types.SpeakerVoiceConfig(\n",
        "                        speaker=speaker2_name,\n",
        "                        voice_config=types.VoiceConfig(\n",
        "                            prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                voice_name=speaker2_voice\n",
        "                            )\n",
        "                        ),\n",
        "                    ),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    file_name = \"dialogue_gemini_tts\"\n",
        "    file_extension = \".wav\"\n",
        "    audio_buffer = b\"\"\n",
        "\n",
        "    for chunk in client_tts.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        if chunk.candidates and chunk.candidates[0].content and chunk.candidates[0].content.parts:\n",
        "            part = chunk.candidates[0].content.parts[0]\n",
        "            if part.inline_data and part.inline_data.data:\n",
        "                data_buffer = part.inline_data.data\n",
        "                if part.inline_data.mime_type and part.inline_data.mime_type.startswith(\"audio/\"):\n",
        "                    if mimetypes.guess_extension(part.inline_data.mime_type) is None:\n",
        "                        data_buffer = convert_to_wav(part.inline_data.data, part.inline_data.mime_type)\n",
        "                audio_buffer += data_buffer\n",
        "\n",
        "    if audio_buffer:\n",
        "        return save_binary_file(f\"{file_name}{file_extension}\", audio_buffer)\n",
        "    else:\n",
        "        print(\"Erreur : aucune donnée audio générée.\")\n",
        "        return None\n",
        "\n",
        "# --- Interface Gradio (Blocks) ---\n",
        "VOICES = [\n",
        "    \"Zephyr\", \"Puck\", \"Achernar\", \"Achird\", \"Algenib\", \"Algieba\", \"Alnilam\",\n",
        "    \"Aoede\", \"Autonoe\", \"Callirrhoe\", \"Charon\", \"Despina\", \"Enceladus\",\n",
        "    \"Erinome\", \"Fenrir\", \"Gacrux\", \"Iapetus\", \"Kore\", \"Laomedeia\", \"Leda\",\n",
        "    \"Orus\", \"Pulcherrima\", \"Rasalgethi\", \"Sadachbia\", \"Sadaltager\", \"Schedar\",\n",
        "    \"Sulafat\", \"Umbriel\", \"Vindemiatrix\", \"Zubenelgenubi\"\n",
        "]\n",
        "with gr.Blocks(title=\"Générateur IA créative\") as demo:\n",
        "    gr.Markdown(\"## Générateur de contenu créatif avec Gemini et Veo\")\n",
        "\n",
        "    # Onglet pour la génération de vidéo\n",
        "    with gr.Tab(\"Vidéo\"):\n",
        "        gr.Markdown(\"### Générateur de vidéo Veo 2.0\")\n",
        "        video_prompt = gr.Textbox(label=\"Entrez une description de la vidéo :\")\n",
        "        video_button = gr.Button(\"Générer la Vidéo\")\n",
        "        video_output = gr.Video(label=\"Vidéo Générée\")\n",
        "\n",
        "        video_button.click(\n",
        "            fn=generate_video,\n",
        "            inputs=video_prompt,\n",
        "            outputs=video_output\n",
        "        )\n",
        "\n",
        "    # Onglet pour la génération de dialogue audio\n",
        "    with gr.Tab(\"Audio\"):\n",
        "        gr.Markdown(\"### Générateur de dialogue Gemini Flash TTS\")\n",
        "        gr.Markdown(\"Entrez le script de l'interlocuteur 1. Le modèle générera une réponse de l'interlocuteur 2.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            speaker1_name_input = gr.Textbox(label=\"Nom Interlocuteur 1\", value=\"Speaker 1\")\n",
        "            speaker2_name_input = gr.Textbox(label=\"Nom Interlocuteur 2\", value=\"Speaker 2\")\n",
        "\n",
        "        with gr.Row():\n",
        "            speaker1_voice_input = gr.Dropdown(label=\"Voix Interlocuteur 1\", choices=VOICES, value=\"Zephyr\")\n",
        "            speaker2_voice_input = gr.Dropdown(label=\"Voix Interlocuteur 2\", choices=VOICES, value=\"Puck\")\n",
        "\n",
        "        dialog_prompt = gr.Textbox(label=\"Texte de l'interlocuteur 1 :\")\n",
        "        audio_button = gr.Button(\"Générer le Dialogue\")\n",
        "        audio_output = gr.Audio(label=\"Dialogue Généré\")\n",
        "\n",
        "        audio_button.click(\n",
        "            fn=generate_tts_dialog,\n",
        "            inputs=[dialog_prompt, speaker1_name_input, speaker2_name_input, speaker1_voice_input, speaker2_voice_input],\n",
        "            outputs=audio_output\n",
        "        )\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "BjQXPTp2DWng",
        "outputId": "8df75cb0-dcdd-41ce-8935-2bb53f6aa568"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b9e390b7ecb27e4c0c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b9e390b7ecb27e4c0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "3ab4e047",
        "outputId": "90dc21a9-507e-4234-adec-e3328c2ce07d"
      },
      "source": [
        "import base64\n",
        "import mimetypes\n",
        "import os\n",
        "import struct\n",
        "import gradio as gr\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration de l'API et du Drive ---\n",
        "# Assurez-vous que 'veo2' est le nom de votre secret dans Colab\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"veo2\")\n",
        "try:\n",
        "    drive.mount(\"/content/drive\")\n",
        "    os.chdir(\"/content/drive/MyDrive/Google AI Studio\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du montage de Drive : {e}\")\n",
        "\n",
        "# Clients pour les deux API\n",
        "client_genai = genai.Client(\n",
        "    http_options={\"api_version\": \"v1beta\"},\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "client_tts = genai.Client(\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "\n",
        "# --- Fonction de génération de vidéo ---\n",
        "MODEL_VEO = \"veo-2.0-generate-001\"\n",
        "video_config = types.GenerateVideosConfig(\n",
        "    aspect_ratio=\"16:9\",\n",
        "    number_of_videos=1,\n",
        "    duration_seconds=8,\n",
        "    person_generation=\"ALLOW_ALL\",\n",
        ")\n",
        "\n",
        "def generate_video(prompt):\n",
        "    operation = client_genai.models.generate_videos(\n",
        "        model=MODEL_VEO,\n",
        "        prompt=prompt,\n",
        "        config=video_config,\n",
        "    )\n",
        "    while not operation.done:\n",
        "        print(\"Vidéo en cours de génération...\")\n",
        "        time.sleep(10)\n",
        "        operation = client_genai.operations.get(operation)\n",
        "\n",
        "    result = operation.result\n",
        "    if not result or not result.generated_videos:\n",
        "        return None\n",
        "\n",
        "    generated_video = result.generated_videos[0]\n",
        "    file_path = \"generated_video.mp4\"\n",
        "    client_genai.files.download(file=generated_video.video)\n",
        "    generated_video.video.save(file_path)\n",
        "    return file_path\n",
        "\n",
        "# --- Fonctions pour le TTS (tirées de votre script) ---\n",
        "def save_binary_file(file_name, data):\n",
        "    f = open(file_name, \"wb\")\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "    return file_name\n",
        "\n",
        "def convert_to_wav(audio_data: bytes, mime_type: str) -> bytes:\n",
        "    parameters = parse_audio_mime_type(mime_type)\n",
        "    bits_per_sample = parameters[\"bits_per_sample\"]\n",
        "    sample_rate = parameters[\"rate\"]\n",
        "    num_channels = 1\n",
        "    data_size = len(audio_data)\n",
        "    bytes_per_sample = bits_per_sample // 8\n",
        "    block_align = num_channels * bytes_per_sample\n",
        "    byte_rate = sample_rate * block_align\n",
        "    chunk_size = 36 + data_size\n",
        "    header = struct.pack(\n",
        "        \"<4sI4s4sIHHIIHH4sI\",\n",
        "        b\"RIFF\",\n",
        "        chunk_size,\n",
        "        b\"WAVE\",\n",
        "        b\"fmt \",\n",
        "        16,\n",
        "        1,\n",
        "        num_channels,\n",
        "        sample_rate,\n",
        "        byte_rate,\n",
        "        block_align,\n",
        "        bits_per_sample,\n",
        "        b\"data\",\n",
        "        data_size\n",
        "    )\n",
        "    return header + audio_data\n",
        "\n",
        "def parse_audio_mime_type(mime_type: str) -> dict[str, int | None]:\n",
        "    bits_per_sample = 16\n",
        "    rate = 24000\n",
        "    parts = mime_type.split(\";\")\n",
        "    for param in parts:\n",
        "        param = param.strip()\n",
        "        if param.lower().startswith(\"rate=\"):\n",
        "            try:\n",
        "                rate_str = param.split(\"=\", 1)[1]\n",
        "                rate = int(rate_str)\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "        elif param.startswith(\"audio/L\"):\n",
        "            try:\n",
        "                bits_per_sample = int(param.split(\"L\", 1)[1])\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "    return {\"bits_per_sample\": bits_per_sample, \"rate\": rate}\n",
        "\n",
        "def generate_tts_dialog(prompt_text, speaker1_name, speaker2_name, speaker1_voice, speaker2_voice):\n",
        "    \"\"\"Génère un dialogue TTS en utilisant l'API Gemini et le stocke.\"\"\"\n",
        "    model = \"gemini-2.5-flash-preview-tts\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=f\"\"\"Read aloud in a neutral, calm voice\n",
        "{speaker1_name}: {prompt_text}\n",
        "{speaker2_name}: (Réponse de l'interlocuteur 2)\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        response_modalities=[\"audio\"],\n",
        "        speech_config=types.SpeechConfig(\n",
        "            multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(\n",
        "                speaker_voice_configs=[\n",
        "                    types.SpeakerVoiceConfig(\n",
        "                        speaker=speaker1_name,\n",
        "                        voice_config=types.VoiceConfig(\n",
        "                            prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                voice_name=speaker1_voice\n",
        "                            )\n",
        "                        ),\n",
        "                    ),\n",
        "                    types.SpeakerVoiceConfig(\n",
        "                        speaker=speaker2_name,\n",
        "                        voice_config=types.VoiceConfig(\n",
        "                            prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                voice_name=speaker2_voice\n",
        "                            )\n",
        "                        ),\n",
        "                    ),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    file_name = \"dialogue_gemini_tts\"\n",
        "    file_extension = \".wav\"\n",
        "    audio_buffer = b\"\"\n",
        "\n",
        "    for chunk in client_tts.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        if chunk.candidates and chunk.candidates[0].content and chunk.candidates[0].content.parts:\n",
        "            part = chunk.candidates[0].content.parts[0]\n",
        "            if part.inline_data and part.inline_data.data:\n",
        "                data_buffer = part.inline_data.data\n",
        "                if part.inline_data.mime_type and part.inline_data.mime_type.startswith(\"audio/\"):\n",
        "                    if mimetypes.guess_extension(part.inline_data.mime_type) is None:\n",
        "                        data_buffer = convert_to_wav(part.inline_data.data, part.inline_data.mime_type)\n",
        "                audio_buffer += data_buffer\n",
        "\n",
        "    if audio_buffer:\n",
        "        return save_binary_file(f\"{file_name}{file_extension}\", audio_buffer)\n",
        "    else:\n",
        "        print(\"Erreur : aucune donnée audio générée.\")\n",
        "        return None\n",
        "\n",
        "# --- Interface Gradio (Blocks) ---\n",
        "VOICES = [\n",
        "    \"Zephyr\", \"Puck\", \"Achernar\", \"Achird\", \"Algenib\", \"Algieba\", \"Alnilam\",\n",
        "    \"Aoede\", \"Autonoe\", \"Callirrhoe\", \"Charon\", \"Despina\", \"Enceladus\",\n",
        "    \"Erinome\", \"Fenrir\", \"Gacrux\", \"Iapetus\", \"Kore\", \"Laomedeia\", \"Leda\",\n",
        "    \"Orus\", \"Pulcherrima\", \"Rasalgethi\", \"Sadachbia\", \"Sadaltager\", \"Schedar\",\n",
        "    \"Sulafat\", \"Umbriel\", \"Vindemiatrix\", \"Zubenelgenubi\"\n",
        "]\n",
        "with gr.Blocks(title=\"Générateur IA créative\") as demo:\n",
        "    gr.Markdown(\"## Générateur de contenu créatif avec Gemini et Veo\")\n",
        "\n",
        "    # Onglet pour la génération de vidéo\n",
        "    with gr.Tab(\"Vidéo\"):\n",
        "        gr.Markdown(\"### Générateur de vidéo Veo 2.0\")\n",
        "        video_prompt = gr.Textbox(label=\"Entrez une description de la vidéo :\")\n",
        "        video_button = gr.Button(\"Générer la Vidéo\")\n",
        "        video_output = gr.Video(label=\"Vidéo Générée\")\n",
        "\n",
        "        video_button.click(\n",
        "            fn=generate_video,\n",
        "            inputs=video_prompt,\n",
        "            outputs=video_output\n",
        "        )\n",
        "\n",
        "    # Onglet pour la génération de dialogue audio\n",
        "    with gr.Tab(\"Audio\"):\n",
        "        gr.Markdown(\"### Générateur de dialogue Gemini Flash TTS\")\n",
        "        gr.Markdown(\"Entrez le script de l'interlocuteur 1. Le modèle générera une réponse de l'interlocuteur 2.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            speaker1_name_input = gr.Textbox(label=\"Nom Interlocuteur 1\", value=\"Speaker 1\")\n",
        "            speaker2_name_input = gr.Textbox(label=\"Nom Interlocuteur 2\", value=\"Speaker 2\")\n",
        "\n",
        "        with gr.Row():\n",
        "            speaker1_voice_input = gr.Dropdown(label=\"Voix Interlocuteur 1\", choices=VOICES, value=\"Zephyr\")\n",
        "            speaker2_voice_input = gr.Dropdown(label=\"Voix Interlocuteur 2\", choices=VOICES, value=\"Puck\")\n",
        "\n",
        "        dialog_prompt = gr.Textbox(label=\"Texte de l'interlocuteur 1 :\")\n",
        "        audio_button = gr.Button(\"Générer le Dialogue\")\n",
        "        audio_output = gr.Audio(label=\"Dialogue Généré\")\n",
        "\n",
        "        audio_button.click(\n",
        "            fn=generate_tts_dialog,\n",
        "            inputs=[dialog_prompt, speaker1_name_input, speaker2_name_input, speaker1_voice_input, speaker2_voice_input],\n",
        "            outputs=audio_output\n",
        "        )\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://96312ab54e3c8530a9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://96312ab54e3c8530a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import mimetypes\n",
        "import os\n",
        "import struct\n",
        "import gradio as gr\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# --- Configuration de l'API et du Drive ---\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"veo2\")\n",
        "\n",
        "try:\n",
        "    drive.mount(\"/content/drive\")\n",
        "    os.chdir(\"/content/drive/MyDrive/Google AI Studio\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du montage de Drive : {e}\")\n",
        "\n",
        "# Clients API\n",
        "client_genai = genai.Client(\n",
        "    http_options={\"api_version\": \"v1beta\"},\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "client_tts = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# --- Fonction de génération de vidéo ---\n",
        "MODEL_VEO = \"veo-2.0-generate-001\"\n",
        "video_config = types.GenerateVideosConfig(\n",
        "    aspect_ratio=\"16:9\",\n",
        "    number_of_videos=1,\n",
        "    duration_seconds=8,\n",
        "    person_generation=\"ALLOW_ALL\",\n",
        ")\n",
        "\n",
        "def generate_video(prompt):\n",
        "    operation = client_genai.models.generate_videos(\n",
        "        model=MODEL_VEO,\n",
        "        prompt=prompt,\n",
        "        config=video_config,\n",
        "    )\n",
        "    while not operation.done:\n",
        "        print(\"Vidéo en cours de génération...\")\n",
        "        time.sleep(10)\n",
        "        operation = client_genai.operations.get(operation)\n",
        "\n",
        "    result = operation.result\n",
        "    if not result or not result.generated_videos:\n",
        "        return None\n",
        "\n",
        "    generated_video = result.generated_videos[0]\n",
        "    file_path = \"generated_video.mp4\"\n",
        "    client_genai.files.download(file=generated_video.video)\n",
        "    generated_video.video.save(file_path)\n",
        "    return file_path\n",
        "\n",
        "# --- Fonctions TTS ---\n",
        "def save_binary_file(file_name, data):\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        f.write(data)\n",
        "    return file_name\n",
        "\n",
        "def convert_to_wav(audio_data: bytes, mime_type: str) -> bytes:\n",
        "    parameters = parse_audio_mime_type(mime_type)\n",
        "    bits_per_sample = parameters[\"bits_per_sample\"]\n",
        "    sample_rate = parameters[\"rate\"]\n",
        "    num_channels = 1\n",
        "    data_size = len(audio_data)\n",
        "    bytes_per_sample = bits_per_sample // 8\n",
        "    block_align = num_channels * bytes_per_sample\n",
        "    byte_rate = sample_rate * block_align\n",
        "    chunk_size = 36 + data_size\n",
        "    header = struct.pack(\n",
        "        \"<4sI4s4sIHHIIHH4sI\",\n",
        "        b\"RIFF\",\n",
        "        chunk_size,\n",
        "        b\"WAVE\",\n",
        "        b\"fmt \",\n",
        "        16,\n",
        "        1,\n",
        "        num_channels,\n",
        "        sample_rate,\n",
        "        byte_rate,\n",
        "        block_align,\n",
        "        bits_per_sample,\n",
        "        b\"data\",\n",
        "        data_size\n",
        "    )\n",
        "    return header + audio_data\n",
        "\n",
        "def parse_audio_mime_type(mime_type: str) -> dict[str, int | None]:\n",
        "    bits_per_sample = 16\n",
        "    rate = 24000\n",
        "    parts = mime_type.split(\";\")\n",
        "    for param in parts:\n",
        "        param = param.strip()\n",
        "        if param.lower().startswith(\"rate=\"):\n",
        "            try:\n",
        "                rate_str = param.split(\"=\", 1)[1]\n",
        "                rate = int(rate_str)\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "        elif param.startswith(\"audio/L\"):\n",
        "            try:\n",
        "                bits_per_sample = int(param.split(\"L\", 1)[1])\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "    return {\"bits_per_sample\": bits_per_sample, \"rate\": rate}\n",
        "\n",
        "def generate_tts_dialog(prompt_text, speaker1_name, speaker2_name, speaker1_voice, speaker2_voice):\n",
        "    model = \"gemini-2.5-flash-preview-tts\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=f\"\"\"Read aloud in a neutral, calm voice\n",
        "{speaker1_name}: {prompt_text}\n",
        "{speaker2_name}: (Réponse de l'interlocuteur 2)\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        response_modalities=[\"audio\"],\n",
        "        speech_config=types.SpeechConfig(\n",
        "            multi_speaker_voice_config=types.MultiSpeakerVoiceConfig(\n",
        "                speaker_voice_configs=[\n",
        "                    types.SpeakerVoiceConfig(\n",
        "                        speaker=speaker1_name,\n",
        "                        voice_config=types.VoiceConfig(\n",
        "                            prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                voice_name=speaker1_voice\n",
        "                            )\n",
        "                        ),\n",
        "                    ),\n",
        "                    types.SpeakerVoiceConfig(\n",
        "                        speaker=speaker2_name,\n",
        "                        voice_config=types.VoiceConfig(\n",
        "                            prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
        "                                voice_name=speaker2_voice\n",
        "                            )\n",
        "                        ),\n",
        "                    ),\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    file_name = \"dialogue_gemini_tts\"\n",
        "    file_extension = \".wav\"\n",
        "    audio_buffer = b\"\"\n",
        "\n",
        "    for chunk in client_tts.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        if chunk.candidates and chunk.candidates[0].content and chunk.candidates[0].content.parts:\n",
        "            part = chunk.candidates[0].content.parts[0]\n",
        "            if part.inline_data and part.inline_data.data:\n",
        "                data_buffer = part.inline_data.data\n",
        "                if part.inline_data.mime_type and part.inline_data.mime_type.startswith(\"audio/\"):\n",
        "                    if mimetypes.guess_extension(part.inline_data.mime_type) is None:\n",
        "                        data_buffer = convert_to_wav(part.inline_data.data, part.inline_data.mime_type)\n",
        "                audio_buffer += data_buffer\n",
        "\n",
        "    if audio_buffer:\n",
        "        return save_binary_file(f\"{file_name}{file_extension}\", audio_buffer)\n",
        "    else:\n",
        "        print(\"Erreur : aucune donnée audio générée.\")\n",
        "        return None\n",
        "\n",
        "# --- Nouvelle fonction : générer mini-dialogue auto ---\n",
        "def generate_dialogue_from_prompt(prompt):\n",
        "    model = \"gemini-1.5-flash\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(\n",
        "                f\"\"\"Invente un très court dialogue (1-2 répliques max) inspiré de cette scène : {prompt}.\n",
        "                Retourne la réponse en JSON avec 'speaker' et 'line'.\"\"\"\n",
        "            )],\n",
        "        )\n",
        "    ]\n",
        "    resp = client_genai.models.generate_content(model=model, contents=contents)\n",
        "    try:\n",
        "        text = resp.candidates[0].content.parts[0].text\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        print(\"Erreur parsing JSON :\", e)\n",
        "        return []\n",
        "\n",
        "# --- Auto dialogue vers TTS ---\n",
        "def auto_dialogue_tts(prompt, voice1=\"Zephyr\", voice2=\"Puck\"):\n",
        "    dialogue = generate_dialogue_from_prompt(prompt)\n",
        "    if not dialogue:\n",
        "        return None\n",
        "\n",
        "    speaker1 = dialogue[0][\"speaker\"] if len(dialogue) > 0 else \"Speaker1\"\n",
        "    line1 = dialogue[0][\"line\"] if len(dialogue) > 0 else \"...\"\n",
        "    speaker2 = dialogue[1][\"speaker\"] if len(dialogue) > 1 else \"Speaker2\"\n",
        "    line2 = dialogue[1][\"line\"] if len(dialogue) > 1 else \"(aucune réponse)\"\n",
        "\n",
        "    script = f\"{speaker1}: {line1}\\n{speaker2}: {line2}\"\n",
        "    return generate_tts_dialog(script, speaker1, speaker2, voice1, voice2)\n",
        "\n",
        "# --- Interface Gradio ---\n",
        "VOICES = [\n",
        "    \"Zephyr\", \"Puck\", \"Achernar\", \"Achird\", \"Algenib\", \"Algieba\", \"Alnilam\",\n",
        "    \"Aoede\", \"Autonoe\", \"Callirrhoe\", \"Charon\", \"Despina\", \"Enceladus\",\n",
        "    \"Erinome\", \"Fenrir\", \"Gacrux\", \"Iapetus\", \"Kore\", \"Laomedeia\", \"Leda\",\n",
        "    \"Orus\", \"Pulcherrima\", \"Rasalgethi\", \"Sadachbia\", \"Sadaltager\", \"Schedar\",\n",
        "    \"Sulafat\", \"Umbriel\", \"Vindemiatrix\", \"Zubenelgenubi\"\n",
        "]\n",
        "with gr.Blocks(title=\"Générateur IA créative\") as demo:\n",
        "    gr.Markdown(\"## Générateur de contenu créatif avec Gemini et Veo\")\n",
        "\n",
        "    # Onglet Vidéo\n",
        "    with gr.Tab(\"Vidéo\"):\n",
        "        gr.Markdown(\"### Générateur de vidéo Veo 2.0\")\n",
        "        video_prompt = gr.Textbox(label=\"Entrez une description de la vidéo :\")\n",
        "        video_button = gr.Button(\"Générer la Vidéo\")\n",
        "        video_output = gr.Video(label=\"Vidéo Générée\")\n",
        "\n",
        "        video_button.click(\n",
        "            fn=generate_video,\n",
        "            inputs=video_prompt,\n",
        "            outputs=video_output\n",
        "        )\n",
        "\n",
        "    # Onglet Audio\n",
        "    with gr.Tab(\"Audio\"):\n",
        "        gr.Markdown(\"### Générateur de dialogue Gemini Flash TTS\")\n",
        "        with gr.Row():\n",
        "            speaker1_name_input = gr.Textbox(label=\"Nom Interlocuteur 1\", value=\"Speaker 1\")\n",
        "            speaker2_name_input = gr.Textbox(label=\"Nom Interlocuteur 2\", value=\"Speaker 2\")\n",
        "\n",
        "        with gr.Row():\n",
        "            speaker1_voice_input = gr.Dropdown(label=\"Voix Interlocuteur 1\", choices=VOICES, value=\"Zephyr\")\n",
        "            speaker2_voice_input = gr.Dropdown(label=\"Voix Interlocuteur 2\", choices=VOICES, value=\"Puck\")\n",
        "\n",
        "        dialog_prompt = gr.Textbox(label=\"Texte de l'interlocuteur 1 :\")\n",
        "        audio_button = gr.Button(\"Générer le Dialogue\")\n",
        "        audio_output = gr.Audio(label=\"Dialogue Généré\")\n",
        "\n",
        "        audio_button.click(\n",
        "            fn=generate_tts_dialog,\n",
        "            inputs=[dialog_prompt, speaker1_name_input, speaker2_name_input, speaker1_voice_input, speaker2_voice_input],\n",
        "            outputs=audio_output\n",
        "        )\n",
        "\n",
        "    # Onglet Mini-Film\n",
        "    with gr.Tab(\"Mini-Film\"):\n",
        "        gr.Markdown(\"### Générateur de mini-cinématique (Veo2 + Dialogue auto TTS)\")\n",
        "\n",
        "        auto_prompt = gr.Textbox(label=\"Entrez votre idée de scène (8s max)\")\n",
        "        auto_voice1 = gr.Dropdown(label=\"Voix Perso 1\", choices=VOICES, value=\"Zephyr\")\n",
        "        auto_voice2 = gr.Dropdown(label=\"Voix Perso 2\", choices=VOICES, value=\"Puck\")\n",
        "\n",
        "        auto_btn = gr.Button(\"Générer Vidéo + Dialogue\")\n",
        "        auto_video_out = gr.Video(label=\"Vidéo générée\")\n",
        "        auto_audio_out = gr.Audio(label=\"Dialogue généré\")\n",
        "\n",
        "        def generate_mini_film(prompt, v1, v2):\n",
        "            vpath = generate_video(prompt)\n",
        "            apath = auto_dialogue_tts(prompt, v1, v2)\n",
        "            return vpath, apath\n",
        "\n",
        "        auto_btn.click(\n",
        "            fn=generate_mini_film,\n",
        "            inputs=[auto_prompt, auto_voice1, auto_voice2],\n",
        "            outputs=[auto_video_out, auto_audio_out]\n",
        "        )\n",
        "\n",
        "# Lancement\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "PL1ynU2YIxHm",
        "outputId": "af5ec286-0209-4cc5-be66-01e5f223c77d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c915845df3e34e7b2c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c915845df3e34e7b2c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}